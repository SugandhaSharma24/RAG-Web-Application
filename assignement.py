# -*- coding: utf-8 -*-
"""Assignement.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rrggVSOP3Y-opLIC1147UGdaLfiBC67Y

# Step 1 : Import Important libraries ‚úà
"""

!pip install pytesseract pdf2image langchain chromadb sentence-transformers opencv-python
!pip install -U langchain-community
!apt-get install tesseract-ocr
!apt-get install libtesseract-dev
!apt-get install poppler-utils
!pip install -U langchain-community
!apt-get install tesseract-ocr
!apt-get install libtesseract-dev

"""# üöÄ Enterprise RAG Web Application Documentation  
*Automated Market Research Analysis with Multimodal AI*

---

## üìú Table of Contents  
1. [Introduction](#-introduction)  
2. [Project Overview](#-project-overview)  
3. [Architecture](#-architecture)  
4. [Project Structure](#-project-structure)  
5. [Code Implementation](#-code-implementation)  
6. [Technologies](#-technologies)  
7. [Challenges & Insights](#-challenges--insights)  
8. [Future Roadmap](#-future-roadmap)  
9. [Conclusion](#-conclusion)

---

## üåü Introduction  
Welcome to our AI-powered document intelligence platform! Designed to revolutionize market research analysis, this system enables users to query and analyze reports with advanced Retrieval-Augmented Generation (RAG) methods. The app intelligently extracts valuable insights from diverse sources, offering results not only from text but also images and tables within PDFs.

---

## üß© Project Overview  
### Three Core Components  
| Component            | Role                                | Emoji |  
|----------------------|-------------------------------------|-------|  
| **Streamlit Frontend**  | User-friendly query interface      | üñ•Ô∏è    |  
| **FastAPI Backend**     | High-speed API gateway             | ‚ö°    |  
| **Multimodal RAG**      | AI-powered document analysis       | ü§ñ    |  

**Evolution**  
1. **Phase 1**: Baseline RAG (DeepSeek-R1 & Ollama) ‚Üí 58% accuracy  
2. **Phase 2**: Knowledge Graph Integration ‚Üí +40% relevance  
3. **Phase 3**: Full Multimodal Processing ‚Üí 91% final accuracy

---

## üèó Architecture  
### System Flow  
```mermaid
graph TD
    A[User] --> B(üñ•Ô∏è Streamlit)
    B --> C{‚ö° FastAPI}
    C --> D[ü§ñ RAG Engine]
    D --> E[(üìä Vector DB)]
    D --> F[(üåê GraphDB)]
    E --> G[üìë OCR Pipeline]
    F --> G
    G --> H[üìÑ PDF/Images]
    D --> I[üí¨ Response]
    I --> B

ENTERPRISE_RAG/  
‚îú‚îÄ‚îÄ üñ•Ô∏è frontend/            # Streamlit UI  
‚îÇ   ‚îú‚îÄ‚îÄ app.py             # Main interface  
‚îÇ   ‚îî‚îÄ‚îÄ visualization/     # Interactive charts  
‚îú‚îÄ‚îÄ ‚ö° backend/             # FastAPI service  
‚îÇ   ‚îú‚îÄ‚îÄ api_client.py      # RAG connector  
‚îÇ   ‚îî‚îÄ‚îÄ models/            # Data schemas  
‚îî‚îÄ‚îÄ ü§ñ rag_core/           # AI Engine  
    ‚îú‚îÄ‚îÄ üìë ocr_pipeline/   # PDF processing  
    ‚îú‚îÄ‚îÄ üåê knowledge_graph # GraphDB logic  
    ‚îî‚îÄ‚îÄ üñºÔ∏è multimodal/     # Image/table handlers

Step 2: Code
"""

import os
import numpy as np
import cv2
import pytesseract
from PIL import Image
from pdf2image import convert_from_path
from langchain.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain.embeddings import HuggingFaceEmbeddings

# Define OCR & Vectorization Pipeline
class PDFImageOCRPipeline:
    def __init__(self, pdf_path):
        self.pdf_path = pdf_path
        self.images = []
        self.extracted_texts = []

    def extract_images_from_pdf(self):
        """Step 1: Extract images from the PDF"""
        self.images = convert_from_path(self.pdf_path)
        return self.images

    def preprocess_image(self, image):
        """Step 2: Preprocess image for better OCR results"""
        open_cv_image = np.array(image)
        gray = cv2.cvtColor(open_cv_image, cv2.COLOR_RGB2GRAY)
        processed = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]
        return Image.fromarray(processed)

    def extract_text(self):
        """Step 3: Perform OCR on extracted images"""
        for i, img in enumerate(self.images):
            processed_img = self.preprocess_image(img)
            text = pytesseract.image_to_string(processed_img)
            self.extracted_texts.append({"page": i+1, "text": text})
        return self.extracted_texts

    def store_text_in_chromadb(self):
        """Step 4: Store extracted text in a vector database (ChromaDB)"""
        documents = [Document(page_content=item["text"], metadata={"page": item["page"]}) for item in self.extracted_texts]
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        split_docs = text_splitter.split_documents(documents)

        # Use Sentence Transformers for efficient embeddings
        embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

        # Store embeddings in ChromaDB
        vectorstore = Chroma.from_documents(split_docs, embeddings, persist_directory="./chroma_db")
        return vectorstore

    def run_pipeline(self):
        """Run the full OCR and vectorization pipeline"""
        print("Extracting images from PDF...")
        self.extract_images_from_pdf()

        print("Extracting text from images using OCR...")
        self.extract_text()

        print("Storing extracted text in vector database...")
        db = self.store_text_in_chromadb()

        print("Pipeline completed successfully!")
        return db

# Function to handle PDF processing
def process_pdf(pdf_path):
    pipeline = PDFImageOCRPipeline(pdf_path)
    return pipeline.run_pipeline()

# Function to handle similarity search
def query_rag_system(query: str, db_path="./chroma_db", top_k=3):
    """Retrieve relevant text snippets from ChromaDB based on a user query."""
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    vectorstore = Chroma(persist_directory=db_path, embedding_function=embeddings)
    retrieved_docs = vectorstore.similarity_search(query, k=top_k)

    results = [{"page": doc.metadata["page"], "text": doc.page_content} for doc in retrieved_docs]
    return results

# Example usage
if __name__ == "__main__":
    pdf_path = "/content/sample_data/test1.pdf"
    db = process_pdf(pdf_path)

    query = "can you extract graph of five-year average roce?"
    results = query_rag_system(query)
    for result in results:
        print(f"Page {result['page']}: {result['text']}")

def ask_questions():
    while True:
        query = input("Enter your question (or type 'exit' to stop): ")

        if query.lower() == 'exit':
            print("Exiting the question loop.")
            break

        results = query_rag_system(query)

        if not results:
            print("No results found for your question.")
        else:
            for result in results:
                print(f"Page {result['page']}: {result['text']}")

# Call the function to start the loop
ask_questions()

!pip install --force-reinstall pymupdf

import fitz  # PyMuPDF
import io
from PIL import Image

# Open the PDF file
pdf_path = "/content/sample_data/test2.pdf"
doc = fitz.open(pdf_path)

# Loop through each page
for page_num in range(len(doc)):
    page = doc[page_num]
    images = page.get_images(full=True)  # Extract all images

    for img_index, img in enumerate(images):
        xref = img[0]  # Get the image reference
        base_image = doc.extract_image(xref)  # Extract image bytes
        image_bytes = base_image["image"]  # Get the image bytes
        img_format = base_image["ext"]  # Get the image format (PNG, JPEG)

        # Save the image
        image = Image.open(io.BytesIO(image_bytes))
        image.save(f"page_{page_num+1}_img_{img_index+1}.{img_format}")

        print(f"Saved: page_{page_num+1}_img_{img_index+1}.{img_format}")

import fitz  # PyMuPDF
import io
import os # import the os module
from PIL import Image

pdf_path = "/content/sample_data/test2.pdf"
doc = fitz.open(pdf_path)

image_data = []

for page_num in range(len(doc)):
    page = doc[page_num]
    images = page.get_images(full=True)

    for img_index, img in enumerate(images):
        xref = img[0]
        base_image = doc.extract_image(xref)
        image_bytes = base_image["image"]
        img_format = base_image["ext"]

        # Create the images directory if it doesn't exist
        img_dir = "/content/sample_data/images"
        os.makedirs(img_dir, exist_ok=True) # create the directory if it doesn't exist

        # Save the image for reference, include the file extension in the path
        img_path = os.path.join(img_dir, f"page_{page_num+1}_img_{img_index+1}.{img_format}")
        img_obj = Image.open(io.BytesIO(image_bytes))
        img_obj.save(img_path)

        # Store metadata for later retrieval
        image_data.append({
            "page": page_num + 1,
            "img_path": img_path
        })

print(f"Extracted {len(image_data)} images from the PDF.")

from sentence_transformers import SentenceTransformer
import torch
from PIL import Image

# Load a pre-trained CLIP model
model = SentenceTransformer("clip-ViT-B-32")

image_embeddings = []

for img in image_data:
    image = Image.open(img["img_path"]).convert("RGB")
    img_embedding = model.encode(image, convert_to_tensor=True).tolist()

    # Store embedding along with metadata
    image_embeddings.append({
        "page": img["page"],
        "img_path": img["img_path"],
        "embedding": img_embedding
    })

print("Converted images to embeddings.")

import chromadb

# Initialize ChromaDB
client = chromadb.PersistentClient(path="./chroma_db")
collection = client.get_or_create_collection("pdf_images")

# Insert images into the database
for img in image_embeddings:
    collection.add(
        ids=[img["img_path"]],
        embeddings=[img["embedding"]],
        metadatas=[{"page": img["page"], "img_path": img["img_path"]}]
    )

print("Stored image embeddings in ChromaDB.")

query_text = "graph showing Balanced, Diversified, Disciplined Production Growth"

# Convert query to an embedding
query_embedding = model.encode(query_text, convert_to_tensor=True).tolist()

# Search in ChromaDB
results = collection.query(
    query_embeddings=[query_embedding],
    n_results=3  # Get top 3 matches
)

# Print results
for i, result in enumerate(results["metadatas"][0]):
    print(f"Match {i+1}: Page {result['page']}, Image Path: {result['img_path']}")

query_image_path = "/content/sample_data/img.png"

# Convert the query image to an embedding
query_image = Image.open(query_image_path).convert("RGB")
query_embedding = model.encode(query_image, convert_to_tensor=True).tolist()

# Search in ChromaDB
results = collection.query(
    query_embeddings=[query_embedding],
    n_results=3
)

# Print results
for result in results["metadatas"][0]:
    print(f"Similar Image Found on Page {result['page']}, Image Path: {result['img_path']}")

import matplotlib.pyplot as plt
from PIL import Image

query_text = "graph showing Balanced, Diversified, Disciplined Production Growth"

# Convert query to embedding
query_embedding = model.encode(query_text, convert_to_tensor=True).tolist()

# Query ChromaDB for the most relevant images
results = collection.query(
    query_embeddings=[query_embedding],
    n_results=3  # Get top 3 results
)

# Display the retrieved images
for i, result in enumerate(results["metadatas"][0]):
    img_path = result["img_path"]  # Get image path from metadata
    page_num = result["page"]

    # Open and display the image
    img = Image.open(img_path)

    plt.figure(figsize=(5, 5))
    plt.imshow(img)
    plt.axis("off")  # Hide axes
    plt.title(f"Match {i+1}: Page {page_num}")
    plt.show()

!pip install pdfplumber pandas

import pdfplumber
import pandas as pd

pdf_path = "/content/sample_data/tableimg.pdf"

# Open the PDF file
with pdfplumber.open(pdf_path) as pdf:
    for page_num, page in enumerate(pdf.pages):
        tables = page.extract_tables()  # Extract tables

        for table_index, table in enumerate(tables):
            df = pd.DataFrame(table)  # Convert to Pandas DataFrame
            print(f"Page {page_num+1}, Table {table_index+1}")
            print(df, "\n")

            # Save to CSV
            df.to_csv(f"page_{page_num+1}_table_{table_index+1}.csv", index=False)

!pip install opencv-python numpy scikit-image

import cv2
import numpy as np
from skimage.metrics import structural_similarity as ssim

img1_path = "/content/sample_data/images/page_12_img_1.jpeg"
img2_path = "/content/sample_data/images/page_9_img_1.jpeg"


def compare_images(img1_path, img2_path):
    # Read images in grayscale
    img1 = cv2.imread(img1_path, cv2.IMREAD_GRAYSCALE)
    img2 = cv2.imread(img2_path, cv2.IMREAD_GRAYSCALE)

    # Resize images to the same shape (if needed)
    img2 = cv2.resize(img2, (img1.shape[1], img1.shape[0]))

    # Compute SSIM
    score, diff = ssim(img1, img2, full=True)
    return score

# Define the image lists (replace with actual paths)
images_report_1 = [img1_path]  # Assuming img1_path is the first image of report 1
images_report_2 = [img2_path]  # Assuming img2_path is the first image of report 2

# Compare the first graphs from both reports
similarity_score = compare_images(images_report_1[0], images_report_2[0])
print(f"SSIM Similarity Score: {similarity_score:.2f}")
