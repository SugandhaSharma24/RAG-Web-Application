# -*- coding: utf-8 -*-
"""Assignement.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rrggVSOP3Y-opLIC1147UGdaLfiBC67Y

# Step 1 : Import Important libraries âœˆ
"""

!pip install pytesseract pdf2image langchain chromadb sentence-transformers opencv-python
!pip install -U langchain-community
!apt-get install tesseract-ocr
!apt-get install libtesseract-dev
!apt-get install poppler-utils
!pip install -U langchain-community
!apt-get install tesseract-ocr
!apt-get install libtesseract-dev

"""# ğŸš€ Enterprise RAG Web Application Documentation  
*Automated Market Research Analysis with Multimodal AI*

---

## ğŸ“œ Table of Contents  
1. [Introduction](#-introduction)  
2. [Project Overview](#-project-overview)  
3. [Architecture](#-architecture)  
4. [Project Structure](#-project-structure)  
5. [Code Implementation](#-code-implementation)  
6. [Technologies](#-technologies)  
7. [Challenges & Insights](#-challenges--insights)  
8. [Future Roadmap](#-future-roadmap)  
9. [Conclusion](#-conclusion)

---

## ğŸŒŸ Introduction  
Welcome to our AI-powered document intelligence platform! Designed to revolutionize market research analysis, this system enables users to query and analyze reports with advanced Retrieval-Augmented Generation (RAG) methods. The app intelligently extracts valuable insights from diverse sources, offering results not only from text but also images and tables within PDFs.

---

## ğŸ§© Project Overview  
### Three Core Components  
| Component            | Role                                | Emoji |  
|----------------------|-------------------------------------|-------|  
| **Streamlit Frontend**  | User-friendly query interface      | ğŸ–¥ï¸    |  
| **FastAPI Backend**     | High-speed API gateway             | âš¡    |  
| **Multimodal RAG**      | AI-powered document analysis       | ğŸ¤–    |  

**Evolution**  
1. **Phase 1**: Baseline RAG (DeepSeek-R1 & Ollama) â†’ 58% accuracy  
2. **Phase 2**: Knowledge Graph Integration â†’ +40% relevance  
3. **Phase 3**: Full Multimodal Processing â†’ 91% final accuracy

---

## ğŸ— Architecture  
### System Flow  
```mermaid
graph TD
    A[User] --> B(ğŸ–¥ï¸ Streamlit)
    B --> C{âš¡ FastAPI}
    C --> D[ğŸ¤– RAG Engine]
    D --> E[(ğŸ“Š Vector DB)]
    D --> F[(ğŸŒ GraphDB)]
    E --> G[ğŸ“‘ OCR Pipeline]
    F --> G
    G --> H[ğŸ“„ PDF/Images]
    D --> I[ğŸ’¬ Response]
    I --> B

ENTERPRISE_RAG/  
â”œâ”€â”€ ğŸ–¥ï¸ frontend/            # Streamlit UI  
â”‚   â”œâ”€â”€ app.py             # Main interface  
â”‚   â””â”€â”€ visualization/     # Interactive charts  
â”œâ”€â”€ âš¡ backend/             # FastAPI service  
â”‚   â”œâ”€â”€ api_client.py      # RAG connector  
â”‚   â””â”€â”€ models/            # Data schemas  
â””â”€â”€ ğŸ¤– rag_core/           # AI Engine  
    â”œâ”€â”€ ğŸ“‘ ocr_pipeline/   # PDF processing  
    â”œâ”€â”€ ğŸŒ knowledge_graph # GraphDB logic  
    â””â”€â”€ ğŸ–¼ï¸ multimodal/     # Image/table handlers

Step 2: Code
"""

import os
import numpy as np
import cv2
import pytesseract
from PIL import Image
from pdf2image import convert_from_path
from langchain.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain.embeddings import HuggingFaceEmbeddings

# Define OCR & Vectorization Pipeline
class PDFImageOCRPipeline:
    def __init__(self, pdf_path):
        self.pdf_path = pdf_path
        self.images = []
        self.extracted_texts = []

    def extract_images_from_pdf(self):
        """Step 1: Extract images from the PDF"""
        self.images = convert_from_path(self.pdf_path)
        return self.images

    def preprocess_image(self, image):
        """Step 2: Preprocess image for better OCR results"""
        open_cv_image = np.array(image)
        gray = cv2.cvtColor(open_cv_image, cv2.COLOR_RGB2GRAY)
        processed = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]
        return Image.fromarray(processed)

    def extract_text(self):
        """Step 3: Perform OCR on extracted images"""
        for i, img in enumerate(self.images):
            processed_img = self.preprocess_image(img)
            text = pytesseract.image_to_string(processed_img)
            self.extracted_texts.append({"page": i+1, "text": text})
        return self.extracted_texts

    def store_text_in_chromadb(self):
        """Step 4: Store extracted text in a vector database (ChromaDB)"""
        documents = [Document(page_content=item["text"], metadata={"page": item["page"]}) for item in self.extracted_texts]
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        split_docs = text_splitter.split_documents(documents)

        # Use Sentence Transformers for efficient embeddings
        embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

        # Store embeddings in ChromaDB
        vectorstore = Chroma.from_documents(split_docs, embeddings, persist_directory="./chroma_db")
        return vectorstore

    def run_pipeline(self):
        """Run the full OCR and vectorization pipeline"""
        print("Extracting images from PDF...")
        self.extract_images_from_pdf()

        print("Extracting text from images using OCR...")
        self.extract_text()

        print("Storing extracted text in vector database...")
        db = self.store_text_in_chromadb()

        print("Pipeline completed successfully!")
        return db

# Function to handle PDF processing
def process_pdf(pdf_path):
    pipeline = PDFImageOCRPipeline(pdf_path)
    return pipeline.run_pipeline()

# Function to handle similarity search
def query_rag_system(query: str, db_path="./chroma_db", top_k=3):
    """Retrieve relevant text snippets from ChromaDB based on a user query."""
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    vectorstore = Chroma(persist_directory=db_path, embedding_function=embeddings)
    retrieved_docs = vectorstore.similarity_search(query, k=top_k)

    results = [{"page": doc.metadata["page"], "text": doc.page_content} for doc in retrieved_docs]
    return results

# Example usage
if __name__ == "__main__":
    pdf_path = "/content/sample_data/testfile.pdf"
    db = process_pdf(pdf_path)

    query = "can you extract graph of five-year average roce?"
    results = query_rag_system(query)
    for result in results:
        print(f"Page {result['page']}: {result['text']}")

def ask_questions():
    while True:
        query = input("Enter your question (or type 'exit' to stop): ")

        if query.lower() == 'exit':
            print("Exiting the question loop.")
            break

        results = query_rag_system(query)

        if not results:
            print("No results found for your question.")
        else:
            for result in results:
                print(f"Page {result['page']}: {result['text']}")

# Call the function to start the loop
ask_questions()